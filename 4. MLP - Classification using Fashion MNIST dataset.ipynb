{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-1RI2LOhzW2"
   },
   "source": [
    "# Problem statement: Create a classification model for the Fashion MNIST\n",
    "\n",
    "The objective is to create a classification model for the Fashion MNIST dataset using a Multi-Layer Perceptron (MLP).\n",
    "\n",
    "We'll follow these steps:\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **Loading the Data**: Fashion MNIST is a dataset of Zalando's article images, with 60,000 training samples and 10,000 test samples. Each sample is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "- **Normalization**: We normalize the pixel values (ranging from 0 to 255) to a scale of 0 to 1. This improves the training efficiency.\n",
    "- **Reshaping for MLP**: Since we are using an MLP, we need to reshape the 28x28 images into a flat array of 784 pixels.\n",
    "\n",
    "### 2. Building the MLP Model\n",
    "- **Dense Layers**: These are fully connected neural layers. The first layer needs to know the input shape (784 in this case).\n",
    "- **Activation Functions**: 'ReLU' is used for non-linear transformations. The final layer uses 'softmax' for a probability distribution over 10 classes.\n",
    "\n",
    "### 3. Compiling the Model\n",
    "- **Optimizer**: 'Adam' is a popular choice for its adaptive learning rate properties.\n",
    "- **Loss Function**: 'sparse_categorical_crossentropy' is suitable for multi-class classification problems.\n",
    "- **Metrics**: We'll use 'accuracy' to understand the performance.\n",
    "\n",
    "### 4. Training the Model\n",
    "- We train the model using the `fit` method, specifying epochs and batch size.\n",
    "\n",
    "### 5. Evaluating the Model\n",
    "- The `evaluate` method is used to test the model on the test set.\n",
    "\n",
    "The notebook contains one exercise in total:\n",
    "\n",
    "* [Exercise 1](#ex_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (0.13.2)\n",
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: tensorflow-macos in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (1.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Collecting notebook\n",
      "  Using cached notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
      "Collecting jupyterlab\n",
      "  Using cached jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
      "Requirement already satisfied: ipykernel in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyter) (6.29.5)\n",
      "Collecting nbconvert\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Collecting jupyter-console\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (58.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.12.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.2.10)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.70.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.13.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.25.6)\n",
      "Requirement already satisfied: wheel~=0.35 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-metal) (0.37.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: namex in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.14.1)\n",
      "Requirement already satisfied: rich in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (8.18.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (1.8.12)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: psutil in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (26.2.1)\n",
      "Requirement already satisfied: appnope in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipykernel->jupyter) (0.1.4)\n",
      "Requirement already satisfied: stack-data in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.19.1)\n",
      "Requirement already satisfied: decorator in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.50)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter) (0.2.13)\n",
      "Collecting jupyterlab-widgets~=3.0.12\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.12\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2>=3.0.3\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyterlab->jupyter) (2.2.1)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jupyter-server<3,>=2.4.0\n",
      "  Downloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
      "\u001b[K     |████████████████████████████████| 385 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-lsp>=2.0.0\n",
      "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting notebook-shim>=0.2\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.7)\n",
      "Requirement already satisfied: anyio in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.8.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Collecting prometheus-client>=0.9\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting terminado>=0.8.3\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: overrides>=5.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Collecting jupyter-events>=0.11.0\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting referencing\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting jsonschema[format-nongpl]>=4.18.0\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.23.1-cp39-cp39-macosx_11_0_arm64.whl (357 kB)\n",
      "\u001b[K     |████████████████████████████████| 357 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Collecting fqdn\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Collecting isoduration\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting uri-template\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting json5>=0.9.0\n",
      "  Downloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting babel>=2.10\n",
      "  Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.1.2-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Downloading cffi-1.17.1-cp39-cp39-macosx_11_0_arm64.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Collecting arrow>=0.15.0\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 5.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting types-python-dateutil>=2.8.10\n",
      "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.2)\n",
      "Requirement already satisfied: pure-eval in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
      "Installing collected packages: rpds-py, referencing, types-python-dateutil, jsonschema-specifications, webencodings, pycparser, jsonschema, fastjsonschema, arrow, webcolors, uri-template, tinycss2, soupsieve, rfc3986-validator, rfc3339-validator, nbformat, isoduration, fqdn, cffi, bleach, terminado, python-json-logger, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, defusedxml, beautifulsoup4, argon2-cffi-bindings, send2trash, prometheus-client, nbconvert, jupyter-server-terminals, jupyter-events, argon2-cffi, jupyter-server, json5, babel, notebook-shim, jupyterlab-server, jupyter-lsp, async-lru, widgetsnbextension, jupyterlab-widgets, jupyterlab, notebook, jupyter-console, ipywidgets, jupyter\n",
      "Successfully installed argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.4 babel-2.17.0 beautifulsoup4-4.13.3 bleach-6.2.0 cffi-1.17.1 defusedxml-0.7.1 fastjsonschema-2.21.1 fqdn-1.5.1 ipywidgets-8.1.5 isoduration-20.11.0 jinja2-3.1.6 json5-0.10.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab-widgets-3.0.13 mistune-3.1.2 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.3.2 notebook-shim-0.2.4 pandocfilters-1.5.1 prometheus-client-0.21.1 pycparser-2.22 python-json-logger-3.3.0 referencing-0.36.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.23.1 send2trash-1.8.3 soupsieve-2.6 terminado-0.18.1 tinycss2-1.4.0 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 widgetsnbextension-4.0.13\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn matplotlib seaborn jupyter tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-macos in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (2.16.2)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (1.2.0)\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.25.6)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.12.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.70.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.26.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.16.2)\n",
      "Requirement already satisfied: packaging in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.15.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.9.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.2.10)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (58.0.4)\n",
      "Requirement already satisfied: wheel~=0.35 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-metal) (0.37.0)\n",
      "Requirement already satisfied: namex in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.0.8)\n",
      "Requirement already satisfied: rich in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.9.4)\n",
      "Requirement already satisfied: optree in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/koksteven/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yKlxud86hcIh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7528 - loss: 0.7212\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 0.4664\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8466 - loss: 0.4438\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8488 - loss: 0.4324\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8488 - loss: 0.4356\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8474 - loss: 0.4354\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8493 - loss: 0.4352\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8493 - loss: 0.4406\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8497 - loss: 0.4340\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8491 - loss: 0.4331\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8323 - loss: 0.4981\n",
      "Test accuracy: 0.830299973487854\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] to improve training convergence\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Reshape data for MLP input\n",
    "# Reshapes each 28x28 image into a 1D array of 784 pixels, \n",
    "# making it compatible with the input layer of the MLP model.\n",
    "train_images = train_images.reshape((-1, 28*28))\n",
    "test_images = test_images.reshape((-1, 28*28))\n",
    "\n",
    "# Build the MLP model\n",
    "# Layer 1: Fully connected (Dense) layer with 128 neurons and ReLU activation.\n",
    "#   - This layer learns high-level features from the input.\n",
    "# Layer 2 (Output layer): Fully connected layer with 10 neurons (one for each class) and softmax activation.\n",
    "#   - Softmax ensures the output values sum to 1, giving probabilities for each class.\n",
    "# Breaking down what is going on:\n",
    "# \t1.\tSequential Model:\n",
    "#       Sequential() defines a stack of layers where data flows one layer at a time.\n",
    "#           It’s a simple way to build feedforward neural networks.\n",
    "#   2. First Layer (Hidden Layer)\n",
    "# \t    Dense Layer: Fully connected layer where every neuron is connected to all previous layer neurons.\n",
    "#       128 neurons: This means the layer has 128 nodes to learn patterns.\n",
    "#       Activation Function (relu):\n",
    "#           “ReLU” stands for Rectified Linear Unit, which introduces non-linearity into the model.\n",
    "#           Formula:  f(x) = \\max(0, x)  (any negative values become 0).\n",
    "#           Why? It helps the network learn complex patterns in the data while avoiding issues \n",
    "#           like the vanishing gradient problem.\n",
    "#       Input Shape (784,):\n",
    "#           The input is a 1D array with 784 values (28×28 image pixels).\n",
    "#           The input shape is only defined in the first layer.\n",
    "#   3. Second layer (Output Layer)\n",
    "#       Dense Layer: Another fully connected layer.\n",
    "#       10 neurons: One neuron for each of the 10 clothing categories in Fashion MNIST.\n",
    "#       Activation Function (softmax):\n",
    "#           Converts raw output numbers into probabilities.\n",
    "#           Each neuron outputs a value between 0 and 1 representing the likelihood of an image belonging to a particular class.\n",
    "#           Example: If an image is of a shirt, the softmax output might look like:\n",
    "#           [0.1, 0.05, 0.02, 0.7, 0.03, 0.05, 0.01, 0.02, 0.01, 0.01]\n",
    "#           Here, the highest probability (0.7) corresponds to the “Shirt” class.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# Optimizer: Adam (adaptive learning rate optimizer).\n",
    "# Loss function: sparse_categorical_crossentropy \n",
    "# (used because the labels are integers instead of one-hot encoded vectors).\n",
    "# Metric: Accuracy (used to track model performance).\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# What is an epoch?\n",
    "# An epoch represents one complete pass through the entire training dataset.\n",
    "#Imagine training is like learning from a textbook:\n",
    "#\t•\tOne Epoch = Reading the entire textbook once.\n",
    "#\t•\tMultiple Epochs = Re-reading the textbook multiple times to improve understanding.\n",
    "#In machine learning:\n",
    "#\t•\tIf we set epochs=10, it means the model will go through the entire dataset 10 times.\n",
    "#\t•\tEach epoch updates the model, gradually improving accuracy.\n",
    "#How to Choose the Number of Epochs?\n",
    "#\t•\tToo few epochs → Model may underfit, meaning it hasn’t learned enough.\n",
    "#\t•\tToo many epochs → Model may overfit, meaning it memorizes training data but performs poorly on new data.\n",
    "#Common approach:\n",
    "#\t1.\tStart with 10-50 epochs.\n",
    "#\t2.\tMonitor accuracy and loss: Stop training if accuracy stops improving.\n",
    "#\t3.\tUse “Early Stopping”: Stop training automatically when performance plateaus.\n",
    "#What is batch size?\n",
    "# Instead of training on the entire dataset at once, we split data into small groups (batches) to speed up learning.\n",
    "#\t•\tBatch size = Number of samples processed before model updates weights.\n",
    "#\t•\tExample:\n",
    "#\t•\t    Dataset: 60,000 images\n",
    "#\t•\t    Batch size = 64\n",
    "#\t•\t    Training will process 64 images at a time, \n",
    "#           then adjust model weights before moving to the next batch.\n",
    "#How to Choose the Batch Size?\n",
    "#\t•\tSmall batch (e.g., 32, 64) → More updates per epoch (better generalization, but slower training).\n",
    "#\t•\tLarge batch (e.g., 128, 256, 512) → Faster training (but may generalize worse).\n",
    "#\t•\tTypical values: 32, 64, 128.\n",
    "#\t•\tPower of 2 (e.g., 32, 64) is often used for faster computation.\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train and Accuracy over Epochs](Training_and_Test_Accuracy_Over_Epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL1TJaEzjvVF"
   },
   "source": [
    "To improve the model's accuracy on the Fashion MNIST dataset, we can experiment with various techniques. Here are some strategies:\n",
    "\n",
    "1. **Increase Model Complexity**: Add more layers or increase the number of neurons in each layer to capture more complex patterns in the data.\n",
    "\n",
    "2. **Regularization**: Implement dropout or L1/L2 regularization to reduce overfitting.\n",
    "\n",
    "3. **Advanced Optimizers**: Experiment with different optimizers like SGD or RMSprop.\n",
    "\n",
    "4. **Learning Rate Scheduling**: Adjust the learning rate during training.\n",
    "\n",
    "5. **Data Augmentation**: Although not typical for MLPs, slight modifications to the input data can make the model more robust.\n",
    "\n",
    "6. **Early Stopping**: Stop training when the validation accuracy stops improving.\n",
    "\n",
    "7. **Hyperparameter Tuning**: Experiment with different activation functions, batch sizes, and epochs.\n",
    "\n",
    "8. **Batch Normalization**: This can help in faster convergence and overall performance improvement.\n",
    "\n",
    "Let's modify the previous code to incorporate some of these strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GkitFh5aij7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.4989 - loss: 1.5956 - val_accuracy: 0.7855 - val_loss: 0.6010\n",
      "Epoch 2/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7091 - loss: 0.8304 - val_accuracy: 0.7999 - val_loss: 0.5559\n",
      "Epoch 3/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7403 - loss: 0.7280 - val_accuracy: 0.8083 - val_loss: 0.5359\n",
      "Epoch 4/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7560 - loss: 0.6839 - val_accuracy: 0.8123 - val_loss: 0.5247\n",
      "Epoch 5/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7660 - loss: 0.6613 - val_accuracy: 0.8142 - val_loss: 0.5164\n",
      "Epoch 6/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7723 - loss: 0.6505 - val_accuracy: 0.8175 - val_loss: 0.5074\n",
      "Epoch 7/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7832 - loss: 0.6207 - val_accuracy: 0.8214 - val_loss: 0.5008\n",
      "Epoch 8/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7849 - loss: 0.6125 - val_accuracy: 0.8218 - val_loss: 0.4965\n",
      "Epoch 9/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7870 - loss: 0.6101 - val_accuracy: 0.8239 - val_loss: 0.4906\n",
      "Epoch 10/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7862 - loss: 0.6074 - val_accuracy: 0.8255 - val_loss: 0.4884\n",
      "Epoch 11/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7882 - loss: 0.5982 - val_accuracy: 0.8284 - val_loss: 0.4842\n",
      "Epoch 12/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7918 - loss: 0.5918 - val_accuracy: 0.8280 - val_loss: 0.4813\n",
      "Epoch 13/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7957 - loss: 0.5795 - val_accuracy: 0.8282 - val_loss: 0.4784\n",
      "Epoch 14/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7951 - loss: 0.5873 - val_accuracy: 0.8279 - val_loss: 0.4783\n",
      "Epoch 15/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7987 - loss: 0.5699 - val_accuracy: 0.8284 - val_loss: 0.4759\n",
      "Epoch 16/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7998 - loss: 0.5685 - val_accuracy: 0.8321 - val_loss: 0.4723\n",
      "Epoch 17/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8006 - loss: 0.5687 - val_accuracy: 0.8316 - val_loss: 0.4724\n",
      "Epoch 18/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8024 - loss: 0.5632 - val_accuracy: 0.8315 - val_loss: 0.4695\n",
      "Epoch 19/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7985 - loss: 0.5684 - val_accuracy: 0.8322 - val_loss: 0.4682\n",
      "Epoch 20/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8037 - loss: 0.5619 - val_accuracy: 0.8350 - val_loss: 0.4659\n",
      "Epoch 21/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8060 - loss: 0.5660 - val_accuracy: 0.8332 - val_loss: 0.4651\n",
      "Epoch 22/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8078 - loss: 0.5514 - val_accuracy: 0.8353 - val_loss: 0.4652\n",
      "Epoch 23/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8082 - loss: 0.5553 - val_accuracy: 0.8344 - val_loss: 0.4633\n",
      "Epoch 24/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.5575 - val_accuracy: 0.8357 - val_loss: 0.4615\n",
      "Epoch 25/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8098 - loss: 0.5516 - val_accuracy: 0.8369 - val_loss: 0.4600\n",
      "Epoch 26/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8097 - loss: 0.5538 - val_accuracy: 0.8372 - val_loss: 0.4593\n",
      "Epoch 27/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.5447 - val_accuracy: 0.8367 - val_loss: 0.4597\n",
      "Epoch 28/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.5473 - val_accuracy: 0.8347 - val_loss: 0.4597\n",
      "Epoch 29/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8093 - loss: 0.5354 - val_accuracy: 0.8373 - val_loss: 0.4583\n",
      "Epoch 30/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8138 - loss: 0.5391 - val_accuracy: 0.8388 - val_loss: 0.4563\n",
      "Epoch 31/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8094 - loss: 0.5433 - val_accuracy: 0.8390 - val_loss: 0.4561\n",
      "Epoch 32/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8101 - loss: 0.5477 - val_accuracy: 0.8382 - val_loss: 0.4550\n",
      "Epoch 33/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.5432 - val_accuracy: 0.8380 - val_loss: 0.4562\n",
      "Epoch 34/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8125 - loss: 0.5404 - val_accuracy: 0.8388 - val_loss: 0.4554\n",
      "Epoch 35/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8075 - loss: 0.5460 - val_accuracy: 0.8394 - val_loss: 0.4538\n",
      "Epoch 36/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8151 - loss: 0.5341 - val_accuracy: 0.8407 - val_loss: 0.4537\n",
      "Epoch 37/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8115 - loss: 0.5447 - val_accuracy: 0.8397 - val_loss: 0.4541\n",
      "Epoch 38/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8083 - loss: 0.5399 - val_accuracy: 0.8385 - val_loss: 0.4546\n",
      "Epoch 39/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8154 - loss: 0.5288 - val_accuracy: 0.8403 - val_loss: 0.4511\n",
      "Epoch 40/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8168 - loss: 0.5301 - val_accuracy: 0.8403 - val_loss: 0.4512\n",
      "Epoch 41/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.5358 - val_accuracy: 0.8393 - val_loss: 0.4515\n",
      "Epoch 42/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.5350 - val_accuracy: 0.8385 - val_loss: 0.4510\n",
      "Epoch 43/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8172 - loss: 0.5285 - val_accuracy: 0.8407 - val_loss: 0.4510\n",
      "Epoch 44/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8157 - loss: 0.5340 - val_accuracy: 0.8418 - val_loss: 0.4495\n",
      "Epoch 45/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8174 - loss: 0.5199 - val_accuracy: 0.8414 - val_loss: 0.4489\n",
      "Epoch 46/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8166 - loss: 0.5296 - val_accuracy: 0.8415 - val_loss: 0.4497\n",
      "Epoch 47/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8136 - loss: 0.5338 - val_accuracy: 0.8410 - val_loss: 0.4497\n",
      "Epoch 48/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8155 - loss: 0.5358 - val_accuracy: 0.8405 - val_loss: 0.4486\n",
      "Epoch 49/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8171 - loss: 0.5350 - val_accuracy: 0.8414 - val_loss: 0.4480\n",
      "Epoch 50/50\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8161 - loss: 0.5283 - val_accuracy: 0.8426 - val_loss: 0.4472\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8382 - loss: 0.4632\n",
      "Test accuracy: 0.8337000012397766\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Modified MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# 1. Batch Normalization\n",
    "#\t•   Applied after a dense layer.\n",
    "#\t•\tNormalizes activations (makes sure neuron outputs have mean ~0 and standard deviation ~1).\n",
    "#\t•\tStabilizes training: Prevents exploding/vanishing gradients.\n",
    "#\t•\tSpeeds up learning by reducing internal covariate shift (shifts in data distribution during training).\n",
    "model.add(BatchNormalization())  # Batch normalization layer\n",
    "# 2. Dropout\n",
    "#\t•\tRandomly disables 50% of neurons during training (Dropout(0.5)).\n",
    "#\t•\tForces the model to learn redundant representations, improving generalization.\n",
    "#\t•\tPrevents overfitting, ensuring neurons don’t become too dependent on specific features.\n",
    "model.add(Dropout(0.5))         # Dropout layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())  # Another batch normalization layer\n",
    "model.add(Dropout(0.5))         # Another dropout layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='SGD', # changed from adam to SGD\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with validation split\n",
    "model.fit(train_images, train_labels, epochs=50, batch_size=64,\n",
    "          validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Difference Between the Two Approaches**\n",
    "\n",
    "Your new model includes **Batch Normalization** and **Dropout**, which were missing in the previous approach. These additions improve the model’s ability to **generalize** and **stabilize learning**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences**\n",
    "\n",
    "| Feature | **Previous Model** | **New Model** |\n",
    "|---------|------------------|--------------|\n",
    "| **Hidden Layer Neurons** | 128 | 256 → 128 |\n",
    "| **Batch Normalization** | ❌ No | ✅ Yes (improves training stability) |\n",
    "| **Dropout** | ❌ No | ✅ Yes (prevents overfitting) |\n",
    "| **Regularization Impact** | Overfits more easily | More resistant to overfitting |\n",
    "| **Training Stability** | Can have unstable gradients | Normalized activations, stable learning |\n",
    "| **Generalization** | Good on training data, but may overfit | Better generalization to test data |\n",
    "| **Test Accuracy** | 0.830299973487854 | 0.8337000012397766 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model is slightly better - the comments below are wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGXeENRJjzVx"
   },
   "source": [
    "The test accuracy decreased slightly in this case. This outcome highlights an important aspect of machine learning: improvements in model architecture don't always lead to better performance, and sometimes simpler models can outperform more complex ones, especially on smaller datasets like Fashion MNIST.\n",
    "\n",
    "Here are a few additional steps you can take to try and improve the model's performance:\n",
    "\n",
    "1. **Adjust the Dropout Rate**: The dropout rate of 0.5 might be too high, causing the model to lose relevant information. Try reducing it to 0.3 or 0.2.\n",
    "\n",
    "2. **Fine-Tune the Model Complexity**: The addition of more neurons might have made the model too complex. Try reducing the number of neurons in the dense layers.\n",
    "\n",
    "3. **Experiment with Different Optimizers**: While Adam is a strong general-purpose optimizer, sometimes others like SGD (with a momentum) or RMSprop might yield better results for specific problems.\n",
    "\n",
    "4. **Modify the Learning Rate**: Adjusting the learning rate of the Adam optimizer could also lead to better results. A lower learning rate with more epochs can sometimes achieve better generalization.\n",
    "\n",
    "5. **Experiment with Batch Sizes**: Smaller or larger batch sizes can impact the model's ability to generalize and learn effectively.\n",
    "\n",
    "6. **Cross-Validation**: Instead of a single validation split, use k-fold cross-validation for a more robust estimate of model performance.\n",
    "\n",
    "Let's adjust the code with some of these suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "T9z3khhbjrVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.6087 - loss: 1.3309 - val_accuracy: 0.8074 - val_loss: 0.5829\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7582 - loss: 0.7640 - val_accuracy: 0.7976 - val_loss: 0.5816\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7722 - loss: 0.6932 - val_accuracy: 0.8115 - val_loss: 0.5368\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7676 - loss: 0.7486 - val_accuracy: 0.8052 - val_loss: 0.6275\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7511 - loss: 0.9232 - val_accuracy: 0.8126 - val_loss: 0.6780\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7384 - loss: 1.1989 - val_accuracy: 0.7776 - val_loss: 1.0666\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7278 - loss: 1.4113 - val_accuracy: 0.7918 - val_loss: 0.7779\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7249 - loss: 1.6426 - val_accuracy: 0.7574 - val_loss: 1.2151\n",
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7163 - loss: 1.9834 - val_accuracy: 0.7458 - val_loss: 1.6729\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7169 - loss: 2.1341 - val_accuracy: 0.7796 - val_loss: 1.3533\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7128 - loss: 2.3521 - val_accuracy: 0.7951 - val_loss: 1.3216\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7101 - loss: 2.6473 - val_accuracy: 0.7954 - val_loss: 1.4339\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7106 - loss: 2.8453 - val_accuracy: 0.7759 - val_loss: 1.5429\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7105 - loss: 3.1087 - val_accuracy: 0.6888 - val_loss: 3.0367\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7177 - loss: 3.1528 - val_accuracy: 0.7997 - val_loss: 1.7957\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7150 - loss: 3.4542 - val_accuracy: 0.7593 - val_loss: 2.5129\n",
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7107 - loss: 3.8552 - val_accuracy: 0.7902 - val_loss: 2.3144\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7171 - loss: 3.9272 - val_accuracy: 0.7602 - val_loss: 3.6605\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7142 - loss: 4.1885 - val_accuracy: 0.6616 - val_loss: 5.8672\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7205 - loss: 4.3208 - val_accuracy: 0.7828 - val_loss: 2.6184\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7194 - loss: 4.4449 - val_accuracy: 0.7837 - val_loss: 3.6956\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7236 - loss: 4.6462 - val_accuracy: 0.7991 - val_loss: 2.9428\n",
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7214 - loss: 4.9900 - val_accuracy: 0.7623 - val_loss: 4.2668\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7143 - loss: 5.0971 - val_accuracy: 0.7781 - val_loss: 3.3134\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7201 - loss: 5.4903 - val_accuracy: 0.7788 - val_loss: 3.5179\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7257 - loss: 5.5239 - val_accuracy: 0.8021 - val_loss: 3.1428\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7212 - loss: 5.8978 - val_accuracy: 0.7983 - val_loss: 3.5680\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7209 - loss: 6.4174 - val_accuracy: 0.7850 - val_loss: 3.7384\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7231 - loss: 6.5357 - val_accuracy: 0.7926 - val_loss: 3.3445\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7234 - loss: 6.9183 - val_accuracy: 0.7639 - val_loss: 5.6736\n",
      "Epoch 9/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7189 - loss: 7.4488 - val_accuracy: 0.7424 - val_loss: 5.8375\n",
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7178 - loss: 8.0524 - val_accuracy: 0.7947 - val_loss: 4.5014\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.7184 - loss: 8.3413 - val_accuracy: 0.7777 - val_loss: 6.0281\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7152 - loss: 8.9575 - val_accuracy: 0.7861 - val_loss: 4.8381\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7173 - loss: 9.3984 - val_accuracy: 0.8015 - val_loss: 5.2994\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7209 - loss: 9.8414 - val_accuracy: 0.8113 - val_loss: 5.1391\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7195 - loss: 10.5324 - val_accuracy: 0.7769 - val_loss: 7.7910\n",
      "Average Validation Accuracy: 0.7670\n"
     ]
    }
   ],
   "source": [
    "# Adjust the model architecture and training parameters\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))         # Reduced dropout rate\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))         # Reduced dropout rate\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model with a modified optimizer\n",
    "#commenting out this one to test k-fold\n",
    "#model.compile(optimizer='RMSprop',  # You can experiment with learning rate here\n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with a different batch size\n",
    "#model.fit(train_images, train_labels, epochs=50, batch_size=32,  # Smaller batch size\n",
    "#          validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_images and train_labels are your data and labels\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert data to numpy arrays if not already\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Initialize lists to store results\n",
    "fold_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(train_images):\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val = train_images[train_index], train_images[val_index]\n",
    "    y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "    # Create a new model instance for each fold\n",
    " #   model = create_model()  # Assuming create_model() is a function that returns a compiled model\n",
    "    model.compile(optimizer='RMSprop',  # You can experiment with learning rate here\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "              validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the validation data\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Average Validation Accuracy: {average_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "#print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmWbPC5gmHZP"
   },
   "source": [
    "The test accuracy has improved to 0.8778, which is a positive outcome. This result indicates that the adjustments made to the model architecture and training parameters were beneficial.\n",
    "\n",
    "However, achieving higher accuracy on a dataset like Fashion MNIST can be challenging, especially with a simple model like a Multi-Layer Perceptron (MLP). To potentially achieve even better results, consider the following additional steps:\n",
    "\n",
    "1. **Feature Engineering**: Although this is more limited with image data and MLPs, ensuring the input data is as informative and clean as possible is crucial.\n",
    "\n",
    "2. **Ensemble Methods**: Combine predictions from several models to improve accuracy. For example, train multiple MLPs with different architectures and average their predictions.\n",
    "\n",
    "3. **Convolutional Neural Networks (CNNs)**: For image data, CNNs are generally more effective than MLPs. They can capture spatial hierarchies in the data better due to their convolutional layers.\n",
    "\n",
    "4. **Hyperparameter Optimization**: Use techniques like grid search or random search to systematically explore different hyperparameter combinations.\n",
    "\n",
    "5. **Advanced Regularization Techniques**: Experiment with other regularization methods like L1 regularization or different dropout configurations.\n",
    "\n",
    "Let's adjust the code with some of these suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BKPmmHqImJEi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koksteven/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.7186 - loss: 0.7679 - val_accuracy: 0.8613 - val_loss: 0.3888\n",
      "Epoch 2/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8681 - loss: 0.3716 - val_accuracy: 0.8672 - val_loss: 0.3689\n",
      "Epoch 3/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8794 - loss: 0.3341 - val_accuracy: 0.8850 - val_loss: 0.3209\n",
      "Epoch 4/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8940 - loss: 0.2981 - val_accuracy: 0.8818 - val_loss: 0.3236\n",
      "Epoch 5/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9005 - loss: 0.2724 - val_accuracy: 0.8917 - val_loss: 0.3083\n",
      "Epoch 6/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9016 - loss: 0.2705 - val_accuracy: 0.8758 - val_loss: 0.3528\n",
      "Epoch 7/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9102 - loss: 0.2506 - val_accuracy: 0.8587 - val_loss: 0.4136\n",
      "Epoch 8/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.9125 - loss: 0.2491 - val_accuracy: 0.8956 - val_loss: 0.3177\n",
      "Epoch 9/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9059 - loss: 0.2648 - val_accuracy: 0.8928 - val_loss: 0.3392\n",
      "Epoch 10/10\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9119 - loss: 0.2564 - val_accuracy: 0.8945 - val_loss: 0.3717\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8882 - loss: 0.3959\n",
      "CNN Test accuracy: 0.8896999955177307\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Reshape data for CNN input\n",
    "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
    "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Build a simple CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(train_images_cnn, train_labels, epochs=10, batch_size=64,\n",
    "              validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
    "\n",
    "print('CNN Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WACMOuHj3wp-"
   },
   "source": [
    "<a name=\"ex_1\"></a>\n",
    "## Exercise 1: Improve the accuracy of the MLP model\n",
    "1. Try different architectures and hyperparameters.\n",
    "2. Use regularization techniques like L1 or L2 regularization.\n",
    "3. Use dropout to reduce overfitting.\n",
    "\n",
    "Referans link: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Try different architectures and hyperparameters\n",
    "\n",
    "| Modification                          | What It Does?                                      | Expected Effect                                      |\n",
    "|---------------------------------------|----------------------------------------------------|------------------------------------------------------|\n",
    "| Increase the number of neurons per layer | Allows the model to learn more complex patterns     | Can improve accuracy, but may cause overfitting      |\n",
    "| Add more hidden layers                | Increases model depth, capturing higher-level features | Improves learning, but training takes longer         |\n",
    "| Use different activation functions (ReLU, LeakyReLU, ELU) | Changes how neurons activate, affecting gradient flow | Helps avoid dead neurons and improves learning       |\n",
    "| Change optimizer (Adam, SGD with momentum, RMSprop) | Affects how weights are updated during training     | Can improve convergence speed & accuracy             |\n",
    "| Use different learning rates          | Controls how fast the model updates weights         | Too high = unstable; too low = slow training         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified model with better architecture and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">571,018</span> (2.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m571,018\u001b[0m (2.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">569,226</span> (2.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m569,226\u001b[0m (2.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a deeper and wider MLP model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer + First hidden layer with 512 neurons\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))  # Increased neurons\n",
    "model.add(BatchNormalization())  # Helps stabilize training by normalizing activations\n",
    "model.add(Dropout(0.3))  # Drops 30% of neurons randomly to prevent overfitting\n",
    "\n",
    "# Second hidden layer with 256 neurons\n",
    "model.add(Dense(256, activation='relu'))  # More neurons than before\n",
    "model.add(BatchNormalization())  \n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer with 128 neurons\n",
    "model.add(Dense(128, activation='relu'))  # Smaller layer for gradual reduction\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer (10 classes for Fashion MNIST)\n",
    "model.add(Dense(10, activation='softmax'))  # Softmax ensures output probabilities sum to 1\n",
    "\n",
    "# Compile the model with Adam optimizer and a tuned learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),  # Lower learning rate for smoother convergence\n",
    "              loss='sparse_categorical_crossentropy',  # Suitable for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Key Improvements:\n",
    "\n",
    "\t•\tIncreased neurons (512 → 256 → 128) for richer feature extraction.\n",
    "\n",
    "\t•\tBatch Normalization: Stabilizes learning and speeds up training.\n",
    "\n",
    "\t•\tDropout (0.3): Prevents overfitting.\n",
    "\n",
    "\t•\tAdam optimizer with learning_rate=0.001 for efficient weight updates.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use L1 & L2 Regularisation (With penalty)\n",
    "\n",
    "| Regularization Type | What It Does?                                   | Effect                                           |\n",
    "|---------------------|-------------------------------------------------|--------------------------------------------------|\n",
    "| L1 Regularization   | Encourages sparse weights (some weights become 0) | Can remove unnecessary connections                |\n",
    "| L2 Regularization   | Penalizes large weights, forcing smaller values | Reduces overfitting by preventing extreme weight values |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">571,018</span> (2.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m571,018\u001b[0m (2.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">569,226</span> (2.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m569,226\u001b[0m (2.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the model with L2 regularization\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with L2 regularization\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)))  # L2 penalty of 0.001\n",
    "model.add(BatchNormalization())  # Stabilizes training\n",
    "model.add(Dropout(0.3))  # Helps prevent overfitting\n",
    "\n",
    "# Second hidden layer with L2 regularization\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer with L2 regularization\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax'))  # Softmax for multi-class classification\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Key Improvements:\n",
    "\n",
    "\t•\tL2 Regularization (l2(0.001)): Forces smaller weights, reducing overfitting.\n",
    "\n",
    "\t•\tBatch Normalization: Stabilizes learning.\n",
    "    \n",
    "\t•\tDropout (0.3): Maintains regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Dropout to Reduce Overfitting\n",
    "\n",
    "🔹 What is Dropout?\n",
    "\t•\tDropout randomly deactivates neurons during training.\n",
    "\t•\tThis prevents the model from relying too much on specific neurons, forcing it to generalize better.\n",
    "\n",
    "🔹 Model with Higher Dropout for Stronger Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">571,018</span> (2.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m571,018\u001b[0m (2.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">569,226</span> (2.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m569,226\u001b[0m (2.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model with stronger dropout\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with 512 neurons and 50% dropout\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Drops 50% of neurons randomly\n",
    "\n",
    "# Second hidden layer with 256 neurons and 50% dropout\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Third hidden layer with 128 neurons and 50% dropout\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax'))  \n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Key Improvements:\n",
    "\t•\tHigher dropout (0.5): Forces more neurons to be inactive, reducing overfitting.\n",
    "\t•\tBatch Normalization: Keeps training stable despite high dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train & Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.6436 - loss: 1.0821 - val_accuracy: 0.7997 - val_loss: 0.5755\n",
      "Epoch 2/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.7789 - loss: 0.6300 - val_accuracy: 0.8189 - val_loss: 0.5242\n",
      "Epoch 3/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.7948 - loss: 0.5921 - val_accuracy: 0.8241 - val_loss: 0.5030\n",
      "Epoch 4/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.7985 - loss: 0.5765 - val_accuracy: 0.8260 - val_loss: 0.4969\n",
      "Epoch 5/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.7971 - loss: 0.5823 - val_accuracy: 0.8270 - val_loss: 0.4942\n",
      "Epoch 6/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8006 - loss: 0.5783 - val_accuracy: 0.8194 - val_loss: 0.5064\n",
      "Epoch 7/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.7999 - loss: 0.5778 - val_accuracy: 0.8274 - val_loss: 0.4971\n",
      "Epoch 8/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.8046 - loss: 0.5702 - val_accuracy: 0.8283 - val_loss: 0.4960\n",
      "Epoch 9/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8035 - loss: 0.5722 - val_accuracy: 0.8281 - val_loss: 0.4941\n",
      "Epoch 10/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.7983 - loss: 0.5877 - val_accuracy: 0.8272 - val_loss: 0.4975\n",
      "Epoch 11/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8015 - loss: 0.5840 - val_accuracy: 0.8299 - val_loss: 0.4927\n",
      "Epoch 12/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.8061 - loss: 0.5712 - val_accuracy: 0.8286 - val_loss: 0.4903\n",
      "Epoch 13/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8030 - loss: 0.5782 - val_accuracy: 0.8301 - val_loss: 0.4901\n",
      "Epoch 14/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8015 - loss: 0.5854 - val_accuracy: 0.8325 - val_loss: 0.4897\n",
      "Epoch 15/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8005 - loss: 0.5820 - val_accuracy: 0.8320 - val_loss: 0.4921\n",
      "Epoch 16/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.7984 - loss: 0.5903 - val_accuracy: 0.8292 - val_loss: 0.4917\n",
      "Epoch 17/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8019 - loss: 0.5785 - val_accuracy: 0.8325 - val_loss: 0.4913\n",
      "Epoch 18/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.7980 - loss: 0.5883 - val_accuracy: 0.8290 - val_loss: 0.4913\n",
      "Epoch 19/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8010 - loss: 0.5854 - val_accuracy: 0.8293 - val_loss: 0.4925\n",
      "Epoch 20/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.7995 - loss: 0.5886 - val_accuracy: 0.8278 - val_loss: 0.4939\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8353 - loss: 0.4827\n",
      "Test Accuracy: 0.8278\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the Fashion MNIST dataset\n",
    "history = model.fit(train_images, train_labels, \n",
    "                    epochs=20, batch_size=64, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model performance on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Training Setup:\n",
    "\t•\tEpochs = 20: More epochs allow the model to learn better.\n",
    "\t•\tBatch size = 64: A balanced choice for speed & generalization.\n",
    "\t•\tValidation set: Helps monitor test accuracy during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "🔹 Higher test accuracy (~84-86%) due to:\n",
    "\n",
    "\t•\tDeeper architecture (more layers & neurons).\n",
    "\n",
    "\t•\tRegularization (L2, Dropout, Batch Norm) prevents overfitting.\n",
    "\n",
    "\t•\tTuned learning rate (Adam optimizer with 0.001).\n",
    "\n",
    "![Training Accuracy Comparison](Training_Accuracy_Comparison.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
